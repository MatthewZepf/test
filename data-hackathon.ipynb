{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3953\n"
     ]
    }
   ],
   "source": [
    "# open chunk_0.csv\n",
    "columns = pd.read_csv(\"chunk_0.csv\", nrows=0).columns.tolist()\n",
    "print(len(columns))  # Should be 1818\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_0.csv...\n",
      "Processing chunk_1.csv...\n",
      "Processing chunk_2.csv...\n",
      "Done! Sample values saved to column_samples.txt ✅\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random  # Import the random module\n",
    "\n",
    "sample_count = 3\n",
    "column_samples = defaultdict(list)\n",
    "\n",
    "\n",
    "num_chunks = 3  # You can increase this if some columns don't fill up\n",
    "for i in range(num_chunks):\n",
    "    chunk_path = f\"chunk_{i}.csv\"\n",
    "    print(f\"Processing {chunk_path}...\")\n",
    "    chunk = pd.read_csv(chunk_path, low_memory=False)\n",
    "    \n",
    "    for col in columns:\n",
    "        current_samples = column_samples[col]\n",
    "        if len(current_samples) < sample_count:\n",
    "            values = chunk[col].dropna().tolist()\n",
    "            if values:\n",
    "                needed = sample_count - len(current_samples)\n",
    "                sampled = random.sample(values, min(needed, len(values)))\n",
    "                column_samples[col].extend(sampled)\n",
    "\n",
    "with open(\"column_samples.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for col in columns:\n",
    "        samples = column_samples[col]\n",
    "        f.write(f\"{col}:\\t{samples}\\n\")\n",
    "\n",
    "print(\"Done! Sample values saved to column_samples.txt ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest = [\"country_name\", \"country_ID\", \"year\", \n",
    "                    \"v2x_polyarchy\", \"v2x_libdem\", \"v2x_partipdem\", \"v2x_delibdem\", \"v2x_egaldem\",\n",
    "                    \"v2elcomvot\", \"v2elsuffrage\", \"v2elbupfin\", \"v2elintim\", \"v2elaccept\", \"v2eltrnout\"\n",
    "                    \"v2elreggov\", \"v2elrgpwr\", \"v2ellocgov\", \"v2ellocpwr\", \"v2ddyrci\", \"v2ddyrrf\", \"v2ddcredal\", \"v2csprtcpt\"\n",
    "                    \"v2mecenefm\", \"v2xed_ed_inpt\", \"e_gdppc\"\n",
    "                    ]  # and so on\n",
    "\n",
    "categorical_cols = [\"country_name\", \"country_ID\", \"v2elcomvot\", \"v2elbupfin\", \"v2elintim\", \"v2elaccept\", \"v2elreggov\", \"v2elrgpwr\", \"v2ellocgov\", \"v2ellocpwr\"\n",
    "                    \"v2ddcredal\", \"v2csprtcpt\", \"v2mecenefm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk_1.csv...\n",
      "Processing chunk_19.csv...\n",
      "Processing chunk_2.csv...\n",
      "Processing chunk_4.csv...\n",
      "Processing chunk_12.csv...\n",
      "Processing chunk_5.csv...\n",
      "Processing chunk_17.csv...\n",
      "Processing chunk_16.csv...\n",
      "Processing chunk_8.csv...\n",
      "Processing chunk_9.csv...\n",
      "Processing chunk_3.csv...\n",
      "Processing chunk_7.csv...\n",
      "Processing chunk_6.csv...\n",
      "Processing chunk_0.csv...\n",
      "Processing chunk_10.csv...\n",
      "Processing chunk_18.csv...\n",
      "Processing chunk_11.csv...\n",
      "Processing chunk_13.csv...\n",
      "Processing chunk_15.csv...\n",
      "Processing chunk_14.csv...\n"
     ]
    }
   ],
   "source": [
    "# start cleaning the data\n",
    "import glob\n",
    "\n",
    "# List of your selected columns\n",
    "cols_of_interest = [\"country_name\", \"country_id\", \"year\", \n",
    "                    \"v2x_polyarchy\", \"v2x_libdem\", \"v2x_partipdem\", \"v2x_delibdem\", \"v2x_egaldem\",\n",
    "                    \"v2elcomvot\", \"v2elsuffrage\", \"v2elpubfin\", \"v2elintim\", \"v2elaccept\", \"v2eltrnout\",\n",
    "                    \"v2elreggov\", \"v2elrgpwr\", \"v2ellocgov\", \"v2ellocpwr\", \"v2ddyrci\", \"v2ddyrrf\", \"v2ddcredal\", \"v2csprtcpt\",\n",
    "                    \"v2mecenefm\"\n",
    "                    ]  # and so on\n",
    "\n",
    "categorical_cols = [\"country_name\", \"country_id\", \"v2elcomvot\", \"v2elpubfin\", \"v2elintim\", \"v2elaccept\", \"v2elreggov\", \"v2elrgpwr\", \"v2ellocgov\", \"v2ellocpwr\",\n",
    "                    \"v2ddcredal\", \"v2csprtcpt\", \"v2mecenefm\"]\n",
    "\n",
    "# Container for cleaned data\n",
    "filtered_chunks = []\n",
    "\n",
    "# Loop over all chunk files, go through each chunk and filter the data\n",
    "# remove any row with a year less than 1980, and remove all columns not in cols_of_interest\n",
    "for chunk_path in glob.glob(\"chunk_*.csv\"):\n",
    "    print(f\"Processing {chunk_path}...\")\n",
    "    chunk = pd.read_csv(chunk_path, low_memory=False)\n",
    "    chunk = chunk[chunk[\"year\"] >= 1980]\n",
    "    chunk = chunk[cols_of_interest]\n",
    "    filtered_chunks.append(chunk)\n",
    "\n",
    "# Concatenate all filtered chunks into a single DataFrame\n",
    "filtered_data = pd.concat(filtered_chunks)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "filtered_data.to_csv(\"filtered_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to grouped_data.csv ✅\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [\n",
    "    col for col in categorical_cols if col not in [\"country_name\", \"year\"]\n",
    "]\n",
    "numeric_cols = [\n",
    "    col for col in cols_of_interest if col not in categorical_cols + [\"country_name\", \"year\"]\n",
    "]\n",
    "# Group and aggregate\n",
    "# Define aggregation functions\n",
    "def mode_or_nan(series):\n",
    "    try:\n",
    "        return series.mode().iloc[0]\n",
    "    except IndexError:\n",
    "        return pd.NA\n",
    "\n",
    "agg_funcs = {col: \"mean\" for col in numeric_cols}\n",
    "for col in categorical_cols:\n",
    "    agg_funcs[col] = mode_or_nan\n",
    "\n",
    "# DO NOT include 'year' or 'country_name' in agg_funcs\n",
    "grouped = filtered_data.groupby([\"year\", \"country_name\"]).agg(agg_funcs).reset_index()\n",
    "\n",
    "\n",
    "# Save to file\n",
    "grouped.to_csv(\"grouped_data.csv\", index=False)\n",
    "print(\"Saved to grouped_data.csv ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
